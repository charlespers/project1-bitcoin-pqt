{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitcoin Project - PQT\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tweepy  \n",
    "import praw\n",
    "import kaggle as kg\n",
    "import nltk\n",
    "import time\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "    # Kaggle\n",
    "KAGGLE_USERNAME = \"\"\n",
    "KAGGLE_KEY = \"\"\n",
    "    # Reddit\n",
    "    \n",
    "YOUR_CLIENT_ID = \"\"\n",
    "YOUR_CLIENT_SERET = \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Kaggle Credentials\n",
    "os.environ['KAGGLE_USERNAME'] = '{KAGGLE_USERNAME}'\n",
    "os.environ['KAGGLE_KEY'] = '{KAGGLE_KEY}'\n",
    "kg.api.authenticate()\n",
    "\n",
    "# Download VADER Sentiment Analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Reddit Credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"YOUR_CLIENT_ID\",\n",
    "    client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "    user_agent=\"YOUR_USER_AGENT\"\n",
    ")\n",
    "\n",
    "#Twitter Credentials\n",
    "client = tweepy.Client(bearer_token=TWITTER_BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mczielinski/bitcoin-historical-data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'on.zip/btcusd_1-min_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m df_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monzip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m kg\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mdataset_download_files(dataset \u001b[38;5;241m=\u001b[39m data_1, path\u001b[38;5;241m=\u001b[39mdf_1, unzip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m df_1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon.zip/btcusd_1-min_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m## Alternativly you can download it directly as a csv:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m''' \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mdata_1 = \"mczielinski/bitcoin-historical-data\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mfile_name = \"btcusd_1-min_data.csv\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mkaggle.api.dataset_download_file(dataset=data_1, file_name=file_name, path=\"./\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mdf_1 = pd.read_csv(file_name)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'on.zip/btcusd_1-min_data.csv'"
     ]
    }
   ],
   "source": [
    "## Dataset 1: Bitcoin Historical Data\n",
    "'''Gets min/min bitcoin data since 2012'''\n",
    "\n",
    "data_1 = \"\"\n",
    "df_1 = \"onzip\"\n",
    "kg.api.dataset_download_files(dataset = data_1, path=df_1, unzip=True)\n",
    "df_1 = pd.read_csv(\"on.zip/btcusd_1-min_data.csv\")\n",
    "\n",
    "## Alternativly you can download it directly as a csv:\n",
    "''' \n",
    "data_1 = \"mczielinski/bitcoin-historical-data\"\n",
    "file_name = \"btcusd_1-min_data.csv\"\n",
    "kaggle.api.dataset_download_file(dataset=data_1, file_name=file_name, path=\"./\")\n",
    "df_1 = pd.read_csv(file_name)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hint W2:\n",
    "# Load the dataset\n",
    "df_test = pd.read_csv(\"btcusd_1-min_data.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df_test[\"timestamp\"] = pd.to_datetime(df_1[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "# Keep only the last 1 year of data\n",
    "df_test = df_test[df_test[\"timestamp\"] >= pd.Timestamp.now() - pd.DateOffset(years=1)]\n",
    "\n",
    "# Save the filtered dataset\n",
    "df_test.to_csv(\"bitcoin_last_year.csv\", index=False)\n",
    "print(\"Dataset reduced to last 1 year and saved as 'bitcoin_last_year.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 2: Bitcoin/Eth Prices\n",
    "'''\n",
    "    Gets day/day bitcoin data from start-Present\n",
    "    *** Note there are some other useful datasets here \n",
    "    Explore!\n",
    "'''\n",
    "data_2 = \"kapturovalexander/bitcoin-and-ethereum-prices-from-start-to-2023\"\n",
    "df_2 = \"on_1.zip\"\n",
    "kg.api.dataset_download_files(dataset = data_2,path=df_2,unzip = True)\n",
    "''' \n",
    "    There are 10 files in the zip: df_2 is currently \n",
    "    set to Eth/USD for the month of January, you can \n",
    "    change that by replacing on_1.zip/{filename.csv}\n",
    "    with the name of the file.\n",
    "'''\n",
    "df_2 = pd.read_csv(\"on_1.zip/ETH-USD (01-05.2024).csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 3: Bitcoin F&G index\n",
    "'''Uses live data to get the fear and greed index of bitcoin'''\n",
    "## get the data (json)\n",
    "def get_fng_data():\n",
    "    url = \"https://api.alternative.me/fng/\"\n",
    "    params = {\"limit\": 0, \"format\": \"json\", \"date_format\": \"world\"}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"]\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return None\n",
    "## json->csv\n",
    "def convert_csv(data, filename):\n",
    "    if not data:\n",
    "        print(\"No data\")\n",
    "        return\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "## Call functions\n",
    "## df_3 = get_fng_data()\n",
    "## convert_csv(df_3,\"fear_greed_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reddit setup\n",
    "auth = requests.auth.HTTPBasicAuth(YOUR_CLIENT_ID,YOUR_CLIENT_SERET)\n",
    "with open('pw.txt','r') as f:\n",
    "    pw = f.read()\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'princeton-pqt',\n",
    "    'password': 'Charfyee3!123'\n",
    "}\n",
    "headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',auth=auth, data=data, headers=headers)\n",
    "## res.json()\n",
    "TOKEN = res.json()['access_token']\n",
    "headers['Authorization'] = f'bearer {TOKEN}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 5: Reddit Scraping\n",
    "res = requests.get('https://oauth.reddit.com/r/Bitcoin/hot',headers=headers ) #/new or params = {'limit':100})\n",
    "res.json()\n",
    "for post in res.json()['data']['children'][:1]:\n",
    "   print(post['data'])\n",
    "\n",
    "\n",
    "post['data'].keys()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = []\n",
    "# Loop through the JSON response and collect relevant fields\n",
    "for post in res.json()['data']['children']:\n",
    "    df5.append({  # Collect data correctly\n",
    "        'subreddit': post['data']['subreddit'],\n",
    "        'title': post['data']['title'],\n",
    "        'selftext': post['data']['selftext'],\n",
    "        'ups': post['data']['ups'],\n",
    "        'downs':post['data']['downs']\n",
    "    })\n",
    "\n",
    "# Create DataFrame from collected data\n",
    "df_5 = pd.DataFrame(df5)  # Now it works!\n",
    "\n",
    "# Save to CSV\n",
    "df_5.to_csv('reddit_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 6: Google Search Trends\n",
    "'''Maps google search trends with keyword 'Bitcoin' '''\n",
    "def fetch_google(keyword, timeframe, max_retries=3):\n",
    "    \"\"\"Fetch Google Search Trends for a given keyword and timeframe, and save to a CSV file with additional insights.\"\"\"\n",
    "    \n",
    "    retry_count = 0  # Track retries\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            print(f\"Fetching Google Trends data for '{keyword}' with timeframe '{timeframe}' (Attempt {retry_count + 1})\")\n",
    "\n",
    "            # Initialize Pytrends with a User-Agent to bypass blocks\n",
    "            pytrends = TrendReq(hl=\"en-US\", tz=360, requests_args={'headers': {'User-Agent': 'Mozilla/5.0'}})\n",
    "\n",
    "            # Build the search payload\n",
    "            pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo=\"US\", gprop=\"\")\n",
    "\n",
    "            # Fetch the interest over time\n",
    "            trends_data = pytrends.interest_over_time()\n",
    "\n",
    "            # Check if data is returned\n",
    "            if trends_data.empty:\n",
    "                print(\"No data returned. Check if the keyword and timeframe are valid.\")\n",
    "                return\n",
    "\n",
    "            # Remove the \"isPartial\" column if it exists\n",
    "            if \"isPartial\" in trends_data.columns:\n",
    "                trends_data.drop(columns=[\"isPartial\"], inplace=True)\n",
    "\n",
    "            # ðŸ”¹ Add useful columns\n",
    "            trends_data[\"Rolling_Avg\"] = trends_data[keyword].rolling(window=7, min_periods=1).mean()  # 7-day moving avg\n",
    "            trends_data[\"Normalized_Score\"] = (trends_data[keyword] - trends_data[keyword].min()) / \\\n",
    "                                              (trends_data[keyword].max() - trends_data[keyword].min()) * 100  # Scale 0-100\n",
    "\n",
    "            # Generate timestamped filename\n",
    "            timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            filename = f\"google_trends_{keyword.lower()}_{timestamp}.csv\"\n",
    "            filepath = os.path.join(os.getcwd(), filename)\n",
    "\n",
    "            # Save to CSV\n",
    "            trends_data.to_csv(filepath)\n",
    "            print(f\"Data successfully saved to '{filepath}'\")\n",
    "            return  # Exit function on success\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching Google Trends data: {e}\")\n",
    "            retry_count += 1\n",
    "            wait_time = 5 * (2 ** retry_count)  # Exponential backoff\n",
    "            print(f\"Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "# Fetch Google Trends data for Bitcoin over the last month\n",
    "fetch_google(\"Bitcoin\", \"today 1-m\")  # Last 30 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
