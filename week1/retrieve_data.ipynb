{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized environemnt:\n",
    "\n",
    "    (1) In the top right click the select kernal.\n",
    "\n",
    "    (2) Choose \"Select another kernal\"\n",
    "\n",
    "    (3) Then \"Python Environment\"\n",
    "\n",
    "    (4) Then \"Create Python Environment\"\n",
    "\n",
    "    (5) Then \"Conda\" \n",
    "\n",
    "    (6) Then \"3.12\"\n",
    "\n",
    "SSL Certifications:\n",
    "\n",
    "    [1] Run the command \"ls ~/.kaggle/kaggle.json\"\n",
    "        Make sure this is in Users/name/.kaggle/kaggle.json\n",
    "\n",
    "    [2] run the command \"pip install --upgrade certifi\"\n",
    "\n",
    "    [3] run the command \"pip install --force-reinstall pygments\"\n",
    "\n",
    "    [4] restart vsCode\n",
    "\n",
    "This will ensure everyone is working on the same python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following commands in your terminal before running this cell:\n",
    "''' 1) Change your directory to week1:\n",
    "        - open a terminal (ctr + `)\n",
    "        - enter the command: cd week1\n",
    "    2) install requirement by running\n",
    "        - pip install -r requirements.txt'''\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import praw\n",
    "import kaggle as kg\n",
    "import nltk\n",
    "import time\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go create our API keys/credentials.\n",
    "\n",
    "Reddit.com -- \n",
    "\n",
    "    (1) Go to reddit.com and make an account.\n",
    "    \n",
    "    (2) Go to Settings -> Privacy -> Third party app authorizations\n",
    "\n",
    "    (3) In the top left click create token.\n",
    "\n",
    "    (4) Enter any name such as \"pqtBitcoin\":\n",
    "        Select Script\n",
    "        Enter any website in redirrect url (you can use google.com)\n",
    "\n",
    "    (5) Copy the secret phrase and store it in the code block below.\n",
    "        Copy the phrase under \"personal use script\" and copy it into your_client_id below\n",
    "\n",
    "Kaggle.com --\n",
    "\n",
    "    (1) Create an account\n",
    "\n",
    "    (2) Navigate to \"Account\" â†’ \"Create New API Token\"\n",
    "\n",
    "    (3) Save this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit \n",
    "YOUR_CLIENT_ID = \"rVTYVH2BwZDj4uGXKO3LXA\"\n",
    "YOUR_CLIENT_SERET = \"BMspeVM0JZ3jAxhp3rlNwW-Za1lYRQ\"\n",
    "\n",
    "# Kaggle\n",
    "KAGGLE_USERNAME = \"charlespersm\"\n",
    "KAGGLE_KEY = \"aceeb0736c999e0ecf7881408ba696e2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll download our first dataset from kaggle.com\n",
    "\n",
    "* Note that this file is intentionally too large to work with, next week we will focus on cleaning our data, but for now focus on the data collection part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 1: Bitcoin Historical Data\n",
    "'''Gets min/min bitcoin data since 2012'''\n",
    "data_1 = \"mczielinski/bitcoin-historical-data\"\n",
    "df_1 = \"onzip\"\n",
    "kg.api.dataset_download_files(dataset = data_1, path=df_1, unzip=True)\n",
    "\n",
    "\n",
    "## Here we download the dataset onto a zipfile.\n",
    "    # You can see the dataset directly by clicking on the file on the left.\n",
    "\n",
    "\n",
    "## Alternativly you can download it directly as a csv:\n",
    "''' \n",
    "data_1 = \"mczielinski/bitcoin-historical-data\"\n",
    "file_name = \"btcusd_1-min_data.csv\"\n",
    "kaggle.api.dataset_download_file(dataset=data_1, file_name=file_name, path=\"./\")\n",
    "df_1 = pd.read_csv(file_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We now can access this file directly and 'look' at it\n",
    "df_1 = pd.read_csv(\"./onzip/btcusd_1-min_data.csv\")\n",
    "\n",
    "#.head(x) will give you the first x rows\n",
    "df_1.head(100)\n",
    "# feel free to try some other commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get one more historical dataset that compares BTC to other currencies\n",
    "We will once again use kaggle for this so the code should look familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 2: Bitcoin/Eth Prices\n",
    "'''\n",
    "    Gets day/day bitcoin data from start-Present\n",
    "    *** Note there are some other useful datasets here \n",
    "    Explore!\n",
    "'''\n",
    "data_2 = \"kapturovalexander/bitcoin-and-ethereum-prices-from-start-to-2023\"\n",
    "df_2 = \"on_1.zip\"\n",
    "kg.api.dataset_download_files(dataset = data_2,path=df_2,unzip = True)\n",
    "''' \n",
    "    There are 10 files in the zip: df_2 is currently \n",
    "    set to Eth/USD for the month of January, you can \n",
    "    change that by replacing on_1.zip/{filename.csv}\n",
    "    with the name of the file.\n",
    "'''\n",
    "df_2 = pd.read_csv(\"on_1.zip/ETH-USD (01-05.2024).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the dataset\n",
    "df_2.head(100)\n",
    "\n",
    "## ALSO note that there are many other datasets in the zip folder on_1.zip\n",
    "# you can use any of these with df = pd.read_csv(\"name.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have historic data, lets try to get a fear and greed index\n",
    "\n",
    "That will give us a normalized score of investor sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 3: Bitcoin F&G index\n",
    "'''Uses live data to get the fear and greed index of bitcoin'''\n",
    "## get the data (json)\n",
    "def get_fng_data():\n",
    "    url = \"https://api.alternative.me/fng/\"\n",
    "    params = {\"limit\": 0, \"format\": \"json\", \"date_format\": \"world\"}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"]\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return None\n",
    "## json->csv\n",
    "def convert_csv(data, filename):\n",
    "    if not data:\n",
    "        print(\"No data\")\n",
    "        return\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "df_3 = get_fng_data()\n",
    "convert_csv(df_3,\"fear_greed_index.csv\")\n",
    "df_3 = pd.read_csv(\"./fear_greed_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run .head() to inspect the new dataset\n",
    "df_3.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now scrape reddit to get any posts that might cause consumer sentiment to change:\n",
    "\n",
    "This will require a bit more setup which we will do below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reddit setup\n",
    "auth = requests.auth.HTTPBasicAuth(YOUR_CLIENT_ID,YOUR_CLIENT_SERET)\n",
    "\n",
    "## Fill in the {username} and {password} with your reddit username/password\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'princeton-pqt',\n",
    "    'password': 'Charfyee3!123'\n",
    "}\n",
    "\n",
    "## This is an arbitrary name\n",
    "headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "## We now get a token which lets us make requests \n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',auth=auth, data=data, headers=headers)\n",
    "res.json()\n",
    "TOKEN = res.json()['access_token']\n",
    "headers['Authorization'] = f'bearer {TOKEN}'\n",
    "print(TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the required credentials to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 5: Reddit Scraping\n",
    "''' Notation: https://oauth.reddit.com/r/Bitcoin/hot\n",
    "    /Bitcoin - this is the subreddit we want you can change this as needed\n",
    "    /hot - this is the type of post you can change this to something like /new if you want newer posts\n",
    "    you can add other parameters such as params={'limit':100} which limits the number of posts to 100\n",
    "'''\n",
    "res = requests.get('https://oauth.reddit.com/r/Bitcoin/hot',headers=headers )\n",
    "res.json()\n",
    "for post in res.json()['data']['children'][:1]:\n",
    "   print(post['data'])\n",
    "\n",
    "\n",
    "post['data'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we printed every post and it's subdata.\n",
    "\n",
    "We also printed the keys which are the possible headers we can construct a dataset with\n",
    "\n",
    "Lets now iterate through res and choose the ones we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = []\n",
    "# Loop through the JSON response and collect relevant fields\n",
    "for post in res.json()['data']['children']:\n",
    "    df5.append({  # Collect data correctly\n",
    "        'subreddit': post['data']['subreddit'],\n",
    "        'title': post['data']['title'],\n",
    "        'selftext': post['data']['selftext'],\n",
    "        'ups': post['data']['ups'],\n",
    "    })\n",
    "\n",
    "# Create DataFrame from collected data\n",
    "df_5 = pd.DataFrame(df5)  # Now it works!\n",
    "\n",
    "# Save to CSV\n",
    "df_5.to_csv('reddit_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the dataset\n",
    "df_5.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Final Datset will be google searches for Bitcoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 6: Google Search Trends\n",
    "'''Maps google search trends with keyword 'Bitcoin' '''\n",
    "def fetch_google(keyword, timeframe, max_retries=3):\n",
    "    \"\"\"Fetch Google Search Trends for a given keyword and timeframe, and save to a CSV file with additional insights.\"\"\"\n",
    "    \n",
    "    retry_count = 0  # Track retries\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            print(f\"Fetching Google Trends data for '{keyword}' with timeframe '{timeframe}' (Attempt {retry_count + 1})\")\n",
    "\n",
    "            # Initialize Pytrends with a User-Agent to bypass blocks\n",
    "            pytrends = TrendReq(hl=\"en-US\", tz=360, requests_args={'headers': {'User-Agent': 'Mozilla/5.0'}})\n",
    "\n",
    "            # Build the search payload\n",
    "            pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo=\"US\", gprop=\"\")\n",
    "\n",
    "            # Fetch the interest over time\n",
    "            trends_data = pytrends.interest_over_time()\n",
    "\n",
    "            # Check if data is returned\n",
    "            if trends_data.empty:\n",
    "                print(\"No data returned. Check if the keyword and timeframe are valid.\")\n",
    "                return\n",
    "\n",
    "            # Remove the \"isPartial\" column if it exists\n",
    "            if \"isPartial\" in trends_data.columns:\n",
    "                trends_data.drop(columns=[\"isPartial\"], inplace=True)\n",
    "\n",
    "            # ðŸ”¹ Add useful columns\n",
    "            trends_data[\"Rolling_Avg\"] = trends_data[keyword].rolling(window=7, min_periods=1).mean()  # 7-day moving avg\n",
    "            trends_data[\"Normalized_Score\"] = (trends_data[keyword] - trends_data[keyword].min()) / \\\n",
    "                                              (trends_data[keyword].max() - trends_data[keyword].min()) * 100  # Scale 0-100\n",
    "\n",
    "            # Generate timestamped filename\n",
    "            timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            filename = f\"google_trends_{keyword.lower()}_{timestamp}.csv\"\n",
    "            filepath = os.path.join(os.getcwd(), filename)\n",
    "\n",
    "            # Save to CSV\n",
    "            trends_data.to_csv(filepath)\n",
    "            print(f\"Data successfully saved to '{filepath}'\")\n",
    "            return  # Exit function on success\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching Google Trends data: {e}\")\n",
    "            retry_count += 1\n",
    "            wait_time = 5 * (2 ** retry_count)  # Exponential backoff\n",
    "            print(f\"Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "# Fetch Google Trends data for Bitcoin over the last month\n",
    "fetch_google(\"Bitcoin\", \"today 1-m\")  # Last 30 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at dataset\n",
    "df_6 = pd.read_csv(\"google_trends_bitcoin_2025-03-02_16-07-28.csv\")\n",
    "df_6.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it! We now have at least 6 datasets that we'll be able to get information for our models from\n",
    "\n",
    "Ideally you're group will go find additional datasets that will make your project unique and more accurate\n",
    "\n",
    "If you have any questions or if the code isn't working please contact me and I'll get back to you as soon as possible\n",
    "\n",
    "cm6268 @ princeton.edu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
